[2024-11-27T02:55:38.019+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-11-27T02:55:38.035+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: google_sheet_to_s3_to_rds.load_to_mysql manual__2024-11-27T02:55:22.833898+00:00 [queued]>
[2024-11-27T02:55:38.044+0000] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: google_sheet_to_s3_to_rds.load_to_mysql manual__2024-11-27T02:55:22.833898+00:00 [queued]>
[2024-11-27T02:55:38.045+0000] {taskinstance.py:2866} INFO - Starting attempt 1 of 1
[2024-11-27T02:55:38.058+0000] {taskinstance.py:2889} INFO - Executing <Task(SQLExecuteQueryOperator): load_to_mysql> on 2024-11-27 02:55:22.833898+00:00
[2024-11-27T02:55:38.063+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=6401) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-11-27T02:55:38.065+0000] {standard_task_runner.py:72} INFO - Started process 6403 to run task
[2024-11-27T02:55:38.066+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'google_sheet_to_s3_to_rds', 'load_to_mysql', 'manual__2024-11-27T02:55:22.833898+00:00', '--job-id', '192', '--raw', '--subdir', 'DAGS_FOLDER/google_sheet_to_s3_to_rds.py', '--cfg-path', '/tmp/tmpiagrj5yt']
[2024-11-27T02:55:38.067+0000] {standard_task_runner.py:105} INFO - Job 192: Subtask load_to_mysql
[2024-11-27T02:55:38.111+0000] {task_command.py:467} INFO - Running <TaskInstance: google_sheet_to_s3_to_rds.load_to_mysql manual__2024-11-27T02:55:22.833898+00:00 [running]> on host 4b3a7f9b2751
[2024-11-27T02:55:38.410+0000] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='google_sheet_to_s3_to_rds' AIRFLOW_CTX_TASK_ID='load_to_mysql' AIRFLOW_CTX_EXECUTION_DATE='2024-11-27T02:55:22.833898+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-27T02:55:22.833898+00:00'
[2024-11-27T02:55:38.411+0000] {logging_mixin.py:190} INFO - Task instance is in running state
[2024-11-27T02:55:38.411+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: queued
[2024-11-27T02:55:38.412+0000] {logging_mixin.py:190} INFO - Current task name:load_to_mysql state:running start_date:2024-11-27 02:55:38.036697+00:00
[2024-11-27T02:55:38.412+0000] {logging_mixin.py:190} INFO - Dag name:google_sheet_to_s3_to_rds and current dag run status:running
[2024-11-27T02:55:38.413+0000] {taskinstance.py:731} INFO - ::endgroup::
[2024-11-27T02:55:38.413+0000] {sql.py:278} INFO - Executing: INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("1", "Analytically Impaired ", "Inconsistent, poor quality, poorly organized", "N/a", "No awareness or interest", "N/a", "", "Few skills, and these attached to specific functions"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("2", "Localized Analytics", "Data useable, but in functional or process silos", "Islands of data, technology, and expertise", "Only at the function or process level", "Multiple disconnected targets that may not be strategically important", "", "Isolated pockets of analysts with no communication"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("3", "Analytical Aspirations", "Organization beginning to create centralized data repository", "Early stages of an enterprise‐wide approach", "Leaders beginning to recognize importance of analytics", "Analytical efforts coalescing behind a small set of targets", "", "Influx of analysts in key target areas"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("4", "Analytical Companies", "Integrated, accurate, common data in central warehouse", "Key data, technology and analysts are centralized or networked", "Leadership support for analytical competence", "Analytical activity centered on a few key domains", "", "Highly capable analysts in central or networked organization"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("5", "Analytical Competitors", "Relentless search for new data and metrics", "All key analytical resources centrally managed", "Strong leadership passion for analytical competition", "Analytics support the firm’s distinctive capability and strategy", "", "World‐class professional analysts and attention to analytical amateurs ");
[2024-11-27T02:55:38.422+0000] {base.py:84} INFO - Retrieving connection 'mysql_conn'
[2024-11-27T02:55:38.428+0000] {base.py:84} INFO - Retrieving connection 'mysql_conn'
[2024-11-27T02:55:38.729+0000] {sql.py:544} INFO - Running statement: INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("1", "Analytically Impaired ", "Inconsistent, poor quality, poorly organized", "N/a", "No awareness or interest", "N/a", "", "Few skills, and these attached to specific functions"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("2", "Localized Analytics", "Data useable, but in functional or process silos", "Islands of data, technology, and expertise", "Only at the function or process level", "Multiple disconnected targets that may not be strategically important", "", "Isolated pockets of analysts with no communication"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("3", "Analytical Aspirations", "Organization beginning to create centralized data repository", "Early stages of an enterprise‐wide approach", "Leaders beginning to recognize importance of analytics", "Analytical efforts coalescing behind a small set of targets", "", "Influx of analysts in key target areas"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("4", "Analytical Companies", "Integrated, accurate, common data in central warehouse", "Key data, technology and analysts are centralized or networked", "Leadership support for analytical competence", "Analytical activity centered on a few key domains", "", "Highly capable analysts in central or networked organization"); INSERT INTO stage.dmm_lookup (Score, Stage, Data, Enterprise, Leadership, Targets, Technology, Analytics) VALUES ("5", "Analytical Competitors", "Relentless search for new data and metrics", "All key analytical resources centrally managed", "Strong leadership passion for analytical competition", "Analytics support the firm’s distinctive capability and strategy", "", "World‐class professional analysts and attention to analytical amateurs ");, parameters: None
[2024-11-27T02:55:38.771+0000] {sql.py:553} INFO - Rows affected: 1
[2024-11-27T02:55:38.838+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-27T02:55:38.839+0000] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=google_sheet_to_s3_to_rds, task_id=load_to_mysql, run_id=manual__2024-11-27T02:55:22.833898+00:00, execution_date=20241127T025522, start_date=20241127T025538, end_date=20241127T025538
[2024-11-27T02:55:38.848+0000] {logging_mixin.py:190} INFO - Task instance in success state
[2024-11-27T02:55:38.848+0000] {logging_mixin.py:190} INFO -  Previous state of the Task instance: running
[2024-11-27T02:55:38.849+0000] {logging_mixin.py:190} INFO - Dag name:google_sheet_to_s3_to_rds queued_at:2024-11-27 02:55:22.845829+00:00
[2024-11-27T02:55:38.849+0000] {logging_mixin.py:190} INFO - Task hostname:4b3a7f9b2751 operator:SQLExecuteQueryOperator
[2024-11-27T02:55:38.883+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-27T02:55:38.903+0000] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-11-27T02:55:38.907+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
